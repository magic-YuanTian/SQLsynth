# Author: Yuan Tian
# Gtihub: https://github.com/magic-YuanTian/SQLsynth


import json
import requests
import copy
import openai_api as openai_api

"""
    Required Input: 
        - database schema
        - ground truth SQL query
        - generated SQL query
    Optional Input: 
        - NL query
        - Rating history
        - description of each column/table
        - execution results of ground truth and generated SQL queries
    
    Output: 
        a score ranging from 0 to 100 to indicate the equivalence of the two queries
    
   Based on prompt engineering, ask LLM to rate the equivalence of two SQL queries. Prompt the LLM to think step by step. Include the rating history to make the rating more consistent.
"""

def get_llm_sql_sql_equivalence_rating_prompt(database_schema, ground_truth_sql_query, generated_sql_query, nl_query=None, rating_history=None, column_description=None, table_description=None, execution_results_ground_truth=None, execution_results_generated=None):
    
    # construct the prompt components
    rating_history_str = f"### Rating on prior tasks\n\n{rating_history}\n### Current task\n\n" if rating_history else ""
    database_schema_str = f">>> Database schema:\n{database_schema}\n" if database_schema else ""
    table_description_str = f">>> Description of each table:\n{table_description}\n" if table_description else ""
    column_description_str = f">>> Description of each column:\n{column_description}\n" if column_description else ""
    nl_query_str = f">>> Ground truth natural language question:\n{nl_query}\n" if nl_query else ""
    ground_truth_sql_query_str = f">>> Ground truth SQL query:\n{ground_truth_sql_query}\n" if ground_truth_sql_query else ""
    generated_sql_query_str = f">>> Generated SQL query:\n{generated_sql_query}\n" if generated_sql_query else ""
    
    # show execution results if both are available
    execution_results_str = ""
    if execution_results_ground_truth and execution_results_generated:
        execution_results_str = f">>> Execution results of ground truth SQL query:\n{execution_results_ground_truth}\n>>> Execution results of generated SQL query:\n{execution_results_generated}\n"

    instruction_str = "### OBJECTIVE\n\nYou are a database expert professional in reading and writing SQL query. Now there are two SQL queries. One is the ground truth The other is generated by human or AI from the ground truth natural language question, which may contain errors. The goal is to analyze and rate the degree of equivalence between the ground truth SQL query and the generated SQL query. Please decompose the task and think step by step. Provide a clear and complete analysis. Based on your analysis, please return an equivalence score indicating how equivalent they are. 100 indicates that you are very confident these two SQL queries are equivalent, while 0 means these two SQL queries are not equivalent at all. Make sure to be consistent with ratings of prior tasks.\n\n### RESPONSE FORMAT\n\nFirst show your step by step analysis. Then return an equivalence score ranging from 0 to 100. Include the score at the specific format:\n<score>Your value here</score>"
    
    prompt = f"{rating_history_str}{database_schema_str}{table_description_str}{column_description_str}{nl_query_str}{ground_truth_sql_query_str}{generated_sql_query_str}{execution_results_str}{instruction_str}"
    
    return prompt


def get_llm_nl_sql_equivalence_rating_prompt(database_schema, generated_sql_query, ground_truth_sql_query=None, nl_query=None, rating_history=None, column_description=None, table_description=None, execution_results_ground_truth=None):
    
    # construct the prompt components
    rating_history_str = f"### Rating on prior tasks\n\n{rating_history}\n### Current task\n\n" if rating_history else ""
    database_schema_str = f"\n>>> Database schema:\n{database_schema}\n" if database_schema else ""
    table_description_str = f"\n>>> Description of each table:\n{table_description}\n" if table_description else ""
    column_description_str = f"\n>>> Description of each column:\n{column_description}\n" if column_description else ""
    nl_query_str = f">>> Ground truth natural language question:\n{nl_query}\n" if nl_query else ""
    ground_truth_sql_query_str = f">>> Ground truth SQL query:\n{ground_truth_sql_query}\n" if ground_truth_sql_query else ""
    generated_sql_query_str = f">>> SQL query:\n{generated_sql_query}\n" if generated_sql_query else ""
    
    # show execution results if both are available
    execution_results_str = ""
    if execution_results_ground_truth:
        execution_results_str = f">>> Execution results of ground truth SQL query:\n{execution_results_ground_truth}\n"

    instruction_str = "\n### OBJECTIVE\nYou are a database expert professional in SQL and relational databases. Now there are a SQL query and its corresponding nautural lanuguage question. However, based the schema (if provided), the natural laugage may not be completely equivalent to the SQL query. Your goal is to analyze and rate the degree of equivalence between the SQL query and the natural language question. If the schema isn't empty or N/A, but the reference columns/tables in SQL is not shown in the schema, the score should not be high. If the schema is empty or N/A, you can loose the criteria and make an assumption. Please decompose the task and think step by step. Provide a clear and complete analysis. Your analysis should base on this specific database schema. Based on your analysis, please return an equivalence score indicating how equivalent they are. 100 indicates that you are very confident the natural language question is equivalent to the SQL query, while 0 means they are not equivalent at all. Make sure to be consistent with ratings of prior tasks.\n\n### RESPONSE FORMAT\nFirst show your step by step analysis. Then return an equivalence score ranging from 0 to 100. Include the score at the specific format:\n<score>Your value here</score>"
    
    prompt = f"{rating_history_str}### CONTEXT FOR CURRENT TASK\n{database_schema_str}{table_description_str}{column_description_str}{nl_query_str}{ground_truth_sql_query_str}{generated_sql_query_str}{execution_results_str}{instruction_str}"
    
    return prompt



# generate the prompt to summarize the analysis from mutliple candidates/trials of rating the equivalence between NL and SQL
def review_summarize_multiple_llm_nl_sql_equivalence_rating_prompt(analysis_history, database_schema, generated_sql_query, ground_truth_sql_query=None, nl_query=None, rating_history=None, column_description=None, table_description=None, execution_results_ground_truth=None):
    
    # construct the prompt components
    rating_history_str = f"### Rating on prior tasks\n\n{rating_history}\n### Current task\n\n" if rating_history else ""
    database_schema_str = f"\n>>> Database schema:\n{database_schema}\n" if database_schema else ""
    table_description_str = f"\n>>> Description of each table:\n{table_description}\n" if table_description else ""
    column_description_str = f"\n>>> Description of each column:\n{column_description}\n" if column_description else ""
    nl_query_str = f">>> Ground truth natural language question:\n{nl_query}\n" if nl_query else ""
    ground_truth_sql_query_str = f">>> Ground truth SQL query:\n{ground_truth_sql_query}\n" if ground_truth_sql_query else ""
    generated_sql_query_str = f">>> SQL query:\n{generated_sql_query}\n" if generated_sql_query else ""
    
    # show execution results if both are available
    execution_results_str = ""
    if execution_results_ground_truth:
        execution_results_str = f">>> Execution results of ground truth SQL query:\n{execution_results_ground_truth}\n"

    instruction_str = "\n### OBJECTIVE\nYou are a database expert professional in SQL and relational databases. Given a SQL query and its corresponding nautural lanuguage question, the objective is to analyze their semantic equivalence based on the databased schema (if provided). The natural laugage may not be completely equivalent to the SQL query. The final goal is to provide a step-by-step analysis and a score (0-100) indicating the degree of equivalence between the SQL query and the natural language question. If the schema isn't empty or N/A, but the reference columns/tables in SQL is not shown in the schema, the score should not be high. If the schema is empty or N/A, you can loose the criteria and make an assumption.\nNow this has been analyzed by multiple candidates. They try their best to provide analysis and the rating score but can be not accurate. Now please review their analysis and provide a final more accurate analysis and with a final rating score.\n### RESPONSE FORMAT\nFirst show your step by step analysis and summarization based on candidate analysis. Then return an equivalence score ranging from 0 to 100. Include the score at the specific format:\n<score>Your value here</score>"
    
    prompt = f"{rating_history_str}### CONTEXT\n{database_schema_str}{table_description_str}{column_description_str}{nl_query_str}{ground_truth_sql_query_str}{generated_sql_query_str}{execution_results_str}\n\n#CANDIDATE ANALYSIS\n\n{analysis_history}{instruction_str}"
    
    return prompt

# get the value from the response in the format of <score>value</score>
def get_score_from_response(response):
    start = response.find("<score>") + len("<score>")
    end = response.find("</score>")
    
    
    
    return response[start:end].replace('%', '').strip()

# def get_schema(file_path="kg_db.json"):
#     with open(file_path, "r") as file:
#         database_schema = json.load(file)
    
#     return database_schema


# Given a SQL query, return the prompt to ask LLM generate the step-by-step explanation of the query
# obsolete
def OLD_get_llm_sql2nl_prompt(sql_query):
    prompt = f"### OBJECTIVE\n\nYou are a database expert of SQL queries and relational databases. Now there is a SQL query that you need to explain step by step. Please provide a concise step-by-step explanation of the SQL query. Make sure to cover all the important components of the query and explain them clearly.\n\n### SQL QUERY\n\n{sql_query}\n\n### RESPONSE FORMAT\n\nPlease provide a detailed step-by-step explanation of the SQL query in a clear and concise manner. Please starts with the step-by-step explanation. Follow a logic order that human can understand. Each step starts by (1), (2) .... Please directly and only provide the step-by-step explanation. Don't say you are happy to explain or anything else. Just provide the explanation. Don't provide any summary. "
    return prompt


# prompt llm to generate the step-by-step explanation of a SQL query
# symbolic + llm
def get_llm_sql2nl_prompt(sql_query, schema='N/A (Please assume the schema)'):
    
    prefix = f"### SCHEMA\n{schema}\n\n### SQL QUERY\n{sql_query}\n\n"
    
    prompt = """
### OBJECTIVE
You are a database expert in SQL and relational databases. Your task is to break down a provided SQL query into multiple SQL clauses based on SQL keywords. 
Sometimes multiple subqueries (a subquery contains only one SELECT keyword) are combined to form a compound SQL query. For example, set operations like UNION, INTERSECT, and EXCEPT can join two subqueries. Additionally, one subquery can refer to another to form a nested SQL query, such as "SELECT col FROM table_1 WHERE col IN (SELECT * FROM table_2)." 
When you encounter a complex SQL query, first break it down into subqueries. Then, within each subquery, decompose it into clauses. If there are dependencies among subqueries, start with the inner ones and then refer to them in the outer queries.
After breaking down a SQL query into SQL clauses with dependencies, describe the functionality of each clause in natural language. Use the provided schema to help you understand and explain each clause. Your description should form a coherent, step-by-step explanation in natural language, making sure it is concise and readable.
Additionally, for each subexpression, provide a sub-natural language question that corresponds to the subexpression. For example, for the subexpression "FROM table_name," the corresponding question is "From where are we selecting the data?"

The ultimate goal is to generate parsed SQL clauses with natural language explanations while maintaining their dependency relationships in JSON format.

### RESPONSE FORMAT
Please return the decomposition in JSON format, below is an example:
Given the input SQL query "SELECT * FROM teacher WHERE teacher.gender = male ORDER BY teacher.age DESC LIMIT 3 INTERSECT SELECT first_name , last_name FROM players WHERE first_name = 'TOM' GROUP BY birth_date HAVING COUNT ( * ) > 1 ORDER BY birth_date LIMIT 1",
please return

```JSON
[
    {
        "number": "Query 1",
        "subquery": "SELECT first_name, last_name FROM players GROUP BY birth_date HAVING COUNT ( * ) > 1 ORDER BY birth_date LIMIT 1",
        "explanation": [
            {
                "subexpression": "FROM teacher",
                "explanation": "In <table>teacher</table>",
                "subNL": "What should we focus on?"
            },
            {
                "subexpression": "WHERE teacher.gender = male",
                "explanation": "Only keep <value>male</value> <column>teachers</column>",
                "subNL": "What information do we have for male teachers?"
            },
            {
                "subexpression": "ORDER BY teacher.age DESC LIMIT 3",
                "explanation": "Return the records for the top <value>3</value> <column>oldest teachers</column>.",
                "subNL": "What information are do the top 3 oldest teachers have?"
            },
            {
                "subexpression": "SELECT *",
                "explanation": "Return all the information",
                "subNL": "What information should we focus on?"
            }
        ],
        "dependency": []
    },
    {
        "number": "Query 2",
        "subquery": "SELECT first_name, last_name FROM players WHERE first_name = 'TOM' GROUP BY birth_date HAVING COUNT ( * ) > 1 ORDER BY birth_date LIMIT 1",
        "explanation": [
            {
                "subexpression": "FROM players",
                "explanation": "In <table>players</table>",
                "subNL": "What should we focus on?"
            },
            {
                "subexpression": "WHERE first_name = 'TOM'",
                "explanation": "Keep the records where <column>the first name</column> is <value>'TOM'</value>",
                "subNL": "Which information do we have for 'TOM'?"
            },
            {
                "subexpression": "GROUP BY birth_date",
                "explanation": "Group the records based on <column>the birth date</column>",
                "subNL": "How should we cluster the records?"
            },
            {
                "subexpression": "HAVING COUNT ( * ) > 1",
                "explanation": "Keep the groups where the number of records is greater than 1",
                "subNL": "What groups have more than one record?"
            },
            {
                "subexpression": "ORDER BY birth_date LIMIT 1",
                "explanation": "Sort the records in ascending order based on <column>the birth date</column>, and return the first record",
                "subNL": "Which is the earliest birth date?"
            },
            {
                "subexpression": "SELECT first_name, last name",
                "explanation": "Return <column>the first name</column> and <column>the last name</column>",
                "subNL": "What full names do we have?"
            }
        ],
        "dependency": []
    },
    {
        "number": "Query 3",
        "subquery": "query_1 INTERSECT query_2",
        "explanation": [
            {
                "subexpression": "query_1 INTERSECT query_2",
                "explanation": "Keep the intersection of first query result and second query result.",
                "subNL": "What is the relationship between the two queries?"
            }
        ],
        "dependency": [
            "query 1",
            "query 2"
        ]
    }
]


```
Make sure you use "<key>" (rather than '<key>') for the keys and values in the JSON format. 
Make sure you use ```JSON at the beginning and ``` at the end to encapsulate your returned data.
Make sure put the SELECT clause at the end of each subquery.
Make sure FROM and JOIN are in the same clause.
Make sure the FROM clause is the first clause in each subquery.
Make sure each subquery starts with SELECT clause.
The subNL is the sub-natural language question that corresponds to the subexpression. Make it clear and concise as real human daily question.
Make sure FROM and JOIN are in the same clause!!!

In the explanation in the JSON, mark speical entities using following tags:
- <table></table> for table names
- <column></column> for column names
- <value></value> for concrete values

Please at least provided by following keywords and orders:
    - FROM ... JOIN ...
    - WHERE
    - GROUP BY
    - HAVING
    - ORDER BY
    - LIMIT
    - SELECT



"""

    return prefix + prompt


# Given the NL question, the sub-questions as well as the step-by-step explanation of a SQL query, map each sub-question/step to the corresponding sub-string of the NL question, and return the prompt to ask LLM to generate the sub-question/step for each sub-string.
# first round, only for analysis
def get_map_subquestion_to_substring_prompt_1(nl_query_chunks, nl_question, sql_query, step, finished_task, schema='N/A (Please assume the schema'):
    
    
    prefix = f"### SCHEMA\n{schema}\n\n### SQL QUERY\n{sql_query}\n\n###FULL NL QUESTION\\n{nl_question}n\n###NL STEP DESCRIPTION\n{step}\n\n###CHUNKS OF NL QUESTION\n{nl_query_chunks}\n\n"
    
    prefix2 = f"FINISHED TASKS\nThis include already finished alignment tasks. Pleae make sure the alignment score is consistent relative to the previous tasks.\n{finished_task}\n\n" if finished_task else ""
    
    prompt = '''
    ### OBJECTIVE
You are a database expert in SQL and relational databases. You are provided the database context including schema and a SQL query. Based on the context, you are provided a natural language translated from the SQL query (but can be possibly wrong). 
Now you are also provided with a step description in natural language of the SQL query.
And you are also provided a list of chunks in the natural language question. 
Your task is to identify chunks that are relevant to this step description. For each relevant chunk, also provide a relevance score ranging from 0 to 100, indicating how relevant the step description is to the chunk. 100 indicates high confidence that the step description is relevant to the chunk, while 0 means they are not related at all.
Please do not be too strict. You should consider the SQL query scenario and contexts. Please associate with all possible related chunks. 


### RESPONSE FORMAT
You should analyze and identify relevant chunk.
It is also possible there is no relevant chunks relevant to this step description. In this case, you just analyze and say there is no relevant chunk. Furthermore, if you think the core information is not covered by the chunks, you can also say the core information is not covered by the chunks. The core information is important. For example, if the value is not covered by the chunks, you should say that there is no related chunks at all. Please pay attention to whether the value is covered by the chunks.
If you think core information is not covered by the chunks, you tell all the related chunks.
For a step corresponding to from step, (e.g., In xxx), you should be very flexible, you only determine whether xxx is covered by the question and find syntatically similar words/chunks.
Please keep in mind, you should only use these chunks to analyze. You should not come up with your own chunks.

    '''
    
    return prefix + prefix2 + prompt 


# second round, formatting the response from the first round
def get_map_subquestion_to_substring_prompt_2(nl_query_chunks, first_round_response, nl_question, sql_query, step, finished_task, schema='N/A (Please assume the schema'):
    # based on the first round response, prompt the LLM to construnct the mapping between step description and sub-string in the NL question, and provide the relevance score as a JSON format
    
    prefix = f"\n###Mapping analysis\\{first_round_response}\n###FULL NL QUESTION\\n{nl_question}n\n###NL STEP DESCRIPTION\n{step}\n\n###CHUNKS OF NL QUESTION\n{nl_query_chunks}\n\n"
    
    prompt = '''
    ### OBJECTIVE
You are a database expert in SQL and relational databases.
Based on a step description of the SQL query, the objective is to identify chunks in the NL question that are relevant to this step description. Now another expert already maps this step to relevant chunks. Please check his analysis.
You task is to validate and summarize this analysis. Then please format it as a JSON format response.


### RESPONSE FORMAT
Based on the previous analysis, you should respond a JSON format response. The key is a chunk string, and the value is a relevance score.
Please keep in mind, the keys can only be one of the chunks provided. You should not come up with you own chunks.
If there is no relevant chunk in the analysis, just return an empty JSON data. 
The relevance score should be an integer between 0 and 100. 
The JSON format should be encapsulated by ```json at the beginning and ``` at the end. Below is an example:

```json
{
    "name" : 25,
    "student" : 91
}
```

    '''
    
    return prefix + prompt
    


# extract the marked question by the sub-question from the response, isolating by ```json and ```
def extract_aligned_question(response):
    start = response.find("```json") + len("```json")
    end = response.find("```", start)
    marked_question = response[start:end].strip()
    
    # make sure the string is in JSON format starting with '{' and ending with '}'
    if marked_question[0] != '{' or marked_question[-1] != '}':
        marked_question = '{' + marked_question + '}'
    
    # convert the string to json
    print('&'*50)
    print(marked_question)
    print('&'*50)
    
    marked_question = json.loads(marked_question)
    
    return marked_question



# given context (SQL query, NL question, schema), return the prompt to update the NL question by injecting/outweighting a certain step
def get_inject_step_prompt(sql_query, nl_question, step_explanation, schema='N/A (Please assume the schema)'):
    
    prefix = f"### SCHEMA\n{schema}\n\n### SQL QUERY\n{sql_query}\n\n### ORIGINAL NL QUESTION\n{nl_question}\n\n### NL EXPLANATION OF THE MISSING STEP\n{step_explanation}\n\n"
    
    prompt = f"""
OBJECTIVE
You are a database expert in SQL and relational databases. You are provided the database context including schema and a SQL query. Based on the context, you are given a natural language translated from the SQL query into a human-readable question, which is possibly wrong.
A prior step-by-step analysis on the SQL query decomposes the SQL query into a step-by-step explanation. However, now one step is regarded as missing in the translated natural language question.
Your task is to inject the information answerd by this step into existing natural language question. First, determine if the information contained in this step is indeed missing from the natural language question. If so, generate a new natural language question by incorporating this step. Ensure the new question is clear, concise, and human-readable.
More specifically, you should incorporate "{step_explanation}" into the original natural language question "{nl_question}". 
Please make sure the new question is similar to the questions that humans ask in real-world scenarios. It is concise, clear, and human-readable.
If you believe the existing natural language question already contains the information from the missing subquestion, you can ignore and return the original natural language question, or paraphrase it to make it more clear.

### RESPONSE FORMAT
Please inlcude the new natural language question in the following format:

<nl_query>Your new natural language question here</nl_query>

    """
    
    return prefix + prompt


# Given the step-by-step explanation of a SQL query, return the prompt to ask LLM to generate a natural language query. This query should be natural and human-readable, just like human daily question.
def get_nl_query_prompt(step_by_step_description, sql, schema, examples):
    
    ### context part
    
    
    sql_prompt = f">>> SQL QUERY\n\n{sql}\n" if sql else ""
    
    schema_prompt = f">>> DATABASE SCHEMA\n\n{schema}\n" if sql else ""
    
    if examples:
        examples_prompt = ">>> RELATED EXAMPLES\n\n" + "\n".join([f"> Example {i+1}:\n{example}\n" for i, example in enumerate(examples)])
    else:
        examples_prompt = ""
    
    step_by_step_description_prompt = f">>> STEP-BY-STEP EXPLANATION & SUB-QUESTIONS\n\n{step_by_step_description}\n\n" if step_by_step_description else ""
    
    #### instruction part
    
    objective_prompt = f"### OBJECTIVE\n\nYou are a database expert of SQL queries and relational databases. Given a SQL query and the corresponding database schema, your task is to translate a SQL query into a natural language question. Please try your best to make the question clear, concise, and human-readable, while excluding any ambiguity or information loss. The format and the tone of the question should be similar to the questions that humans ask in real-world scenarios. You are also provided related information that is helpful for understanding the SQL query, such as the step by step natural language explanation of the query. To make the query more natural, you are provided several examples that are related to this SQL query. Furthermore, descriptions of each column or table may be provided. Please try to understand these as they can help you formulate more natural questions. Try to avoid formulating the natural language question in a logic similar to the SQL query."
    
    format_prompt = f"### RESPONSE FORMAT\n\nPlease generate a natural language question that corresponds to the SQL query based on related information. The question should be very natural and similar to a question that real human can ask. It should be a question that a human would ask in a real world. Do NOT include any SQL keywords or technical terms in the question. For example, try to avoid including tables, columns, and other terms in the generated question.  Do NOT include a abbreviated entity name that human won't include in daily talk. You should try to paraphrase each entity according to their meanings. You can make some analysis. But make sure include your final natural language question in the following format:\n<nl_query>Your natural language question here</nl_query>\n\nDo not use 'table' and 'column' in the question! Do not use any SQL keywords in the question! Do not include any SQL related logitcs such as group by, order by, etc. Just make it in human daily verbal style."

    final_prompt = f"### CONTEXT\n\n{sql_prompt}{schema_prompt}{step_by_step_description_prompt}{examples_prompt}{objective_prompt}{format_prompt}"

    return final_prompt

def get_nl2clause_entailment_rating_prompt(database_schema, SQL_clause, nl_explanation=None, nl_query=None, rating_history=None, column_description=None, table_description=None):
    
    # construct the prompt components
    rating_history_str = f"### Rating on prior tasks\n\n{rating_history}\n\nPlease be consistent to your prior rating criteria!\n\n" if rating_history else ""
    database_schema_str = f"\n>>> Database schema:\n{database_schema}\n" if database_schema else ""
    table_description_str = f"\n>>> Description of each table:\n{table_description}\n" if table_description else ""
    column_description_str = f"\n>>> Description of each column:\n{column_description}\n" if column_description else ""
    nl_query_str = f">>> Ground truth natural language question:\n{nl_query}\n" if nl_query else ""
    SQL_clause_str = f">>> SQL clause:\n{SQL_clause}\n" if SQL_clause else ""
    nl_explanation_str = f">>> A potential natural language explanation of the SQL clause:\n{nl_explanation}\n" if nl_explanation else ""
    

    instruction_str = "\n### OBJECTIVE\nYou are a database expert of SQL queries and relational databases. You are provided with a database schema, a natural language question, and a SQL clause (component) extracted from a target SQL query. The target SQL may or may not completely match the natural language question. The ultimate goal is to evaluate the equivalence between the natural language question and the target SQL query under this specific database schema. To ahcieve this ultimate goal, now please only focus on evaluating whether the SQL clause can be entailed by the natural language question. In other words, your current task is to determine whether this SQL clause provides part of the answer to the natural language question, or if the natural language question can be answered by a SQL query containing the SQL clause. To avoid misleading, you are now shown with the target SQL query, but only shown with the SQL clause.\nProvide a thorough and clear analysis based on this specific database schema. Based on your analysis, provide an equivalence score indicating their match level. A score of 100 indicates high confidence that the SQL clause is relevant to the natural language question , while 0 means they are not related at all.If the schema is not empty or N/A, and the reference columns/tables in SQL are not shown in the schema, the score should be considerably low. If the schema is empty or N/A, you can relax the criteria and make assumptions. Please break down the task and think step by step.\n\n### RESPONSE FORMAT\nFirst, detail your reasoning and rating criteria step by step. Based on your analysis, return a relevance score ranging from 0 to 100. Include the score in the following format:\n<score>Your value here</score>"
    
    prompt = f"{rating_history_str}### CONTEXT FOR CURRENT TASK\n{database_schema_str}{table_description_str}{column_description_str}{nl_query_str}{SQL_clause_str}{nl_explanation_str}{instruction_str}"
    
    return prompt

# def get_SQL2subquestion_entailment_rating_prompt(database_schema, SQL, nl_sub_question, rating_history=None, column_description=None, table_description=None):
    
#     # construct the prompt components
#     rating_history_str = f"### Rating on prior tasks\n\n{rating_history}\n\nPlease be consistent to your prior rating criteria!\n\n### Current task\n\n" if rating_history else ""
#     database_schema_str = f"\n>>> Database schema:\n{database_schema}\n" if database_schema else ""
#     table_description_str = f"\n>>> Description of each table:\n{table_description}\n" if table_description else ""
#     column_description_str = f"\n>>> Description of each column:\n{column_description}\n" if column_description else ""
#     nl_sub_question_str = f">>> Sub question:\n{nl_sub_question}\n" if nl_sub_question else ""
#     SQL_str = f">>> SQL query:\n{SQL}\n" if SQL else ""
    
    

#     instruction_str = "d"
    
#     prompt = f"{rating_history_str}### CONTEXT FOR CURRENT TASK\n{database_schema_str}{table_description_str}{column_description_str}{nl_query_str}{SQL_clause_str}{nl_explanation_str}{instruction_str}"
    
#     return prompt


# Given a DB schema and a NL question, decompose the NL question into sub-questions, and return a list of sub-questions
def get_decompose_nl_question_to_sub_questions_prompt(database_schema, nl_query):
    
    database_schema_str = f"\n>>> Database schema:\n{database_schema}\n" if database_schema else ""
    nl_query_str = f"\n>>> Ground truth natural language question:\n{nl_query}\n" if nl_query else ""

    
    instruction_str = """
### OBJECTIVE
You are a database expert of SQL queries and relational databases.
Given a database schema and a natural language question, your task is to decompose the natural language question into smaller, manageable sub-questions. The decomposition is based on the database schema, where each sub-question can be answered by certain data in the database.

### RESPONSE FORMAT
Please list the sub-questions in the following JSON format:
```json
[
    {
        "id": "1",
        "task_description": "Describe the task to be performed.",
        "related_table": "Name of the table related to this task",
        "related_fields": ["List of fields involved in this task"]
    },
    {
        "id": "2",
        "task_description": "Another task description.",
        "related_table": "Another table name",
        "related_fields": ["Field1", "Field2"]
    },
    // other sub-questions...
]
```
"""

    prompt = f"### CONTEXT\n{database_schema_str}{nl_query_str}{instruction_str}"
    
    return prompt




def parse_sub_questions_str_to_json(response_text):
    # extract content between "```json" and "```"" of response_text
    start = response_text.find("```json") + len("```json")
    end = response_text.find("```", start)
    sub_questions_str = response_text[start:end].strip()
    # convert the string to json
    sub_questions = json.loads(sub_questions_str)
    return sub_questions


def get_openai_response_personal(prompt):
    """
    Sends a message to the OpenAI API and retrieves the response.

    Parameters:
    - user_message: str. The message from the user to send to OpenAI.

    Returns:
    - message: str. The response from OpenAI.
    """
    openai_api_key = "sk-proj-1QIYxJojHOHne9p1aDhlT3BlbkFJnKhhKfA5uGCCVp7wwmAj"
    if openai_api_key is None:
        raise ValueError("OpenAI API key is not set in environment variables.")

    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {openai_api_key}"
    }
    data = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    }

    response = requests.post(url, headers=headers, json=data)
    response_data = response.json()
    
    if response.status_code == 200:
        return response_data['choices'][0]['message']['content']
    else:
        raise Exception(f"Error from OpenAI API: {response.status_code} {response.text}")




######################## Useful wrapper ########################

def nl_2_sub_questions(database_schema, nl_query, ):
    prompt = get_decompose_nl_question_to_sub_questions_prompt(database_schema, nl_query)
    print('Prompt for decomposing NL question to sub-questions:')
    print(prompt)
    print('-'*50)
    
    response = get_openai_response_personal(prompt)
    print('Response from LLM:')
    print(response)
    print('-'*50)
    
    json_data = parse_sub_questions_str_to_json(response)
    
    # return a formatted list of sub-questions as string, each line starts with a number (e.g., "(1)")
    sub_questions_str = ''
    for sub_question in json_data:
        sub_questions_str += f"({sub_question['id']}) {sub_question['task_description']}\n\n"
    
    return sub_questions_str